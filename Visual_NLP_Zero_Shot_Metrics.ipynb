{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6b0bdc-0ca6-4905-a7d6-d60c7181797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "license = \"\"\n",
    "\n",
    "if license and \"json\" in license:\n",
    "\n",
    "    with open(license, \"r\") as creds_in:\n",
    "        creds = json.loads(creds_in.read())\n",
    "\n",
    "        for key in creds.keys():\n",
    "            os.environ[key] = creds[key]\n",
    "else:\n",
    "    raise Exception(\"License JSON File is not specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b204384-1915-4d75-9ac3-ad6d0147871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q https://pypi.johnsnowlabs.com/$SPARK_OCR_SECRET/spark-ocr/spark_ocr-6.0.0-py3-none-any.whl\n",
    "\n",
    "!pip install --upgrade -q https://pypi.johnsnowlabs.com/$SECRET/spark-nlp-jsl/spark_nlp_jsl-6.0.0-py3-none-any.whl\n",
    "\n",
    "!pip install -q spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "!pip install -q pandas\n",
    "\n",
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71290960-caf1-43e1-b898-a9e777be3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESTART SESSION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e6d8dd-a007-4a8f-b7a4-f8343b0cf2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkocr import start\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "license = \"\"\n",
    "\n",
    "if license and \"json\" in license:\n",
    "\n",
    "    with open(license, \"r\") as creds_in:\n",
    "        creds = json.loads(creds_in.read())\n",
    "\n",
    "        for key in creds.keys():\n",
    "            os.environ[key] = creds[key]\n",
    "else:\n",
    "    raise Exception(\"License JSON File is not specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf30f3-dde8-443f-954e-ede9666ae62a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_configurations = {\n",
    "    \"spark.extraListeners\": \"com.johnsnowlabs.license.LicenseLifeCycleManager\", #required\n",
    "    \"spark.sql.legacy.allowUntypedScalaUDF\" : \"true\", #required\n",
    "    \"spark.executor.instances\" : \"7\", #change as per system\n",
    "    \"spark.executor.cores\" : \"16\", #change as per system\n",
    "    \"spark.executor.memory\" : \"130G\", #change as per system\n",
    "    \"spark.driver.memory\" : \"100G\", #change as per system\n",
    "    \"spark.sql.shuffle.partitions\" : \"896\" #change as per system\n",
    "}\n",
    "\n",
    "# Not needed for Google Collab\n",
    "os.environ['JAVA_HOME'] = '/home/linuxbrew/.linuxbrew/Cellar/openjdk@17/17.0.15'\n",
    "\n",
    "spark = start(secret=os.environ.get(\"SPARK_OCR_SECRET\"),\n",
    "              nlp_secret=os.environ.get(\"SECRET\"),\n",
    "              nlp_internal=True,\n",
    "              nlp_jsl=True,\n",
    "              nlp_version=os.environ.get(\"PUBLIC_VERSION\"),\n",
    "              extra_conf=extra_configurations)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55ae85b-cbd0-4f1e-9117-a40a6c4acdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Pyspark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Necessary imports from Spark OCR library\n",
    "import sparkocr\n",
    "from sparkocr import start\n",
    "from sparkocr.transformers import *\n",
    "from sparkocr.enums import *\n",
    "from sparkocr.utils import *\n",
    "\n",
    "# import sparknlp packages\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "from collections import Counter\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e9d347-b75f-4f7f-998c-a775d4a8de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(SOURCE_GT_PATH, DF_SAVE_PATH, SAVE_MAPPING_PATH):\n",
    "    \"\"\"\n",
    "    Method to Calculate Precision, Recall and F1-Score\n",
    "    Saves final file with prediction, ground truth, precision, recall\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_metrics(preds, gts):\n",
    "      gt_counter = Counter(gts)\n",
    "      pred_counter = Counter(preds)\n",
    "\n",
    "      tp = 0\n",
    "      for item in pred_counter:\n",
    "          if item in gt_counter:\n",
    "              tp += min(pred_counter[item], gt_counter[item])\n",
    "\n",
    "      fp = sum(pred_counter.values()) - tp\n",
    "      fn = sum(gt_counter.values()) - tp\n",
    "\n",
    "      precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "      recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "\n",
    "      return precision, recall\n",
    "\n",
    "    with open(SOURCE_GT_PATH, \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "\n",
    "    df_predictions = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "\n",
    "    predictions_by_file = {}\n",
    "\n",
    "    for row in df_predictions.select(\"path\").distinct().toLocalIterator():\n",
    "        file_path = row.asDict()[\"path\"]\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        if filename not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        extracted_results = []\n",
    "        rows = df_predictions.filter(F.col(\"path\") == file_path).select(\"positions_ner\")\n",
    "\n",
    "        for r in rows.toLocalIterator():\n",
    "            for ner in r.asDict()[\"positions_ner\"]:\n",
    "                extracted_results.append(ner.asDict()[\"result\"])\n",
    "\n",
    "        predictions_by_file[filename] = extracted_results\n",
    "\n",
    "    summary = {}\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    for filename, predictions in predictions_by_file.items():\n",
    "        gt_values = ground_truth[filename]\n",
    "        gt_values = [i.replace(\"-year-old\", \"\") for i in gt_values]\n",
    "        predictions = [i.replace(\"-year-old\", \"\") for i in predictions]\n",
    "        precision, recall = calculate_metrics([i.replace(\" \", \"\") for i in predictions], \n",
    "                                              [i.replace(\" \", \"\") for i in gt_values])\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        summary[filename] = {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"gt\": gt_values,\n",
    "            \"pred\": predictions\n",
    "        }\n",
    "\n",
    "        print(f\"Filename: {filename} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    avg_precision = round(sum(all_precisions) / len(all_precisions), 4)\n",
    "    avg_recall = round(sum(all_recalls) / len(all_recalls), 4)\n",
    "    f1_score = round(2 * (avg_precision * avg_recall) / (avg_precision + avg_recall), 4)\n",
    "\n",
    "    print(f\"\\nOverall Precision: {avg_precision}\")\n",
    "    print(f\"Overall Recall: {avg_recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "    with open(SAVE_MAPPING_PATH, \"w\") as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    print(f\"Mapping File Saved To : {SAVE_MAPPING_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c844ba4-c91b-45ca-a485-734e70dfc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_image = PdfToImage() \\\n",
    "  .setInputCol(\"content\") \\\n",
    "  .setSplitNumBatch(10) \\\n",
    "  .setOutputCol(\"image_raw\") \\\n",
    "  .setImageType(ImageType.TYPE_3BYTE_BGR) \\\n",
    "  .setSplittingStategy(SplittingStrategy.FIXED_NUMBER_OF_PARTITIONS)\n",
    "\n",
    "ocr = ImageToText() \\\n",
    "    .setInputCol(\"image_raw\") \\\n",
    "    .setOutputCol(\"text\") \\\n",
    "    .setIgnoreResolution(False) \\\n",
    "    .setPageIteratorLevel(PageIteratorLevel.SYMBOL) \\\n",
    "    .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) \\\n",
    "    .setWithSpaces(True) \\\n",
    "    .setKeepLayout(False) \\\n",
    "    .setConfidenceThreshold(70)\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink_full\") \n",
    "\n",
    "abbreviations = ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n",
    "sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setImpossiblePenultimates(abbreviations) \\\n",
    "    .setUseCustomBoundsOnly(False) \\\n",
    "    .setSplitLength(2147483647) \\\n",
    "    .setExplodeSentences(False)\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setContextChars(['.', ',', ';', ':', '!', '?', '*', '\"', \"'\"])\n",
    "\n",
    "zero_shot_ner = ZeroShotNerModel.pretrained(\"zero_shot_ner_roberta\", \"en\", \"clinical/models\")\\\n",
    "    .setEntityDefinitions(\n",
    "        {\n",
    "            \"NAME\" : [\"What is the Patient Name?\"],\n",
    "            \"DATE\" : [\"What is the Patient Date of Birth?\", \"What is the DOB?\"],\n",
    "            \"IDNUM\" : [\"What is the SSN Number?\", \"What is the SSN?\"]\n",
    "        })\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"zero_shot_ner\")\\\n",
    "    .setPredictionThreshold(0.1)\n",
    "\n",
    "ner_converter = NerConverterInternal()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"zero_shot_ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\") \\\n",
    "    .setThreshold(0.60)\n",
    "\n",
    "deid_obfuscated = DeIdentification() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner_chunk\"]) \\\n",
    "    .setOutputCol(\"obfuscated\") \\\n",
    "    .setMode(\"obfuscate\") \\\n",
    "    .setKeepMonth(True) \\\n",
    "    .setKeepYear(True) \\\n",
    "    .setObfuscateDate(True) \\\n",
    "    .setSameEntityThreshold(0.7) \\\n",
    "    .setKeepTextSizeForObfuscation(True) \\\n",
    "    .setFakerLengthOffset(2) \\\n",
    "    .setReturnEntityMappings(True) \\\n",
    "    .setDays(2) \\\n",
    "    .setMappingsColumn(\"aux\") \\\n",
    "    .setIgnoreRegex(True) \\\n",
    "    .setGroupByCol(\"path\") \\\n",
    "    .setRegion(\"us\") \\\n",
    "    .setSeed(40) \\\n",
    "    .setConsistentObfuscation(True) \\\n",
    "    .setChunkMatching({\"NAME\" : 0.60, \"DATE\" : 0.60, \"IDNUM\" : 0.60})\n",
    "\n",
    "cleaner = NerOutputCleaner() \\\n",
    "    .setInputCol(\"aux\") \\\n",
    "    .setOutputCol(\"new_aux\") \\\n",
    "    .setOutputNerCol(\"positions_ner\")\n",
    "\n",
    "position_finder = PositionFinder() \\\n",
    "    .setInputCols(\"positions_ner\") \\\n",
    "    .setOutputCol(\"coordinates\") \\\n",
    "    .setPageMatrixCol(\"positions\")\n",
    "\n",
    "draw_regions = ImageDrawRegions() \\\n",
    "  .setInputCol(\"image_raw\") \\\n",
    "  .setInputRegionsCol(\"coordinates\") \\\n",
    "  .setRectColor(Color.black) \\\n",
    "  .setFilledRect(True) \\\n",
    "  .setOutputCol(\"image_with_regions\")\n",
    "\n",
    "stages = [\n",
    "    pdf_to_image,\n",
    "    ocr,\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    zero_shot_ner,\n",
    "    ner_converter,\n",
    "    deid_obfuscated,\n",
    "    cleaner,\n",
    "    position_finder,\n",
    "    draw_regions\n",
    "]\n",
    "\n",
    "pipe = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3844ed-4062-4527-8268-18e6fc3705fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PDF_PATH = \"/workspace/PDF_FILES_MEDIUM/*\"\n",
    "DF_SAVE_PATH = \"/workspace/zero_shot/\"\n",
    "SOURCE_GT_PATH = \"/workspace/pdf_deid_zero_shot_gts.json\"\n",
    "SAVE_MAPPING_PATH = \"/workspace/zero_shot_result_mapping.json\"\n",
    "SAVE_OUTPUT_PDF = \"/workspace/zero_shot_pdf_output/\"\n",
    "\n",
    "os.makedirs(SAVE_OUTPUT_PDF, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5403decd-7b9b-45cf-95f8-532cecc04257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 09:31:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Warning: Version of org.bytedeco:leptonica could not be found.      (0 + 8) / 8]\n",
      "Warning: Version of org.bytedeco:tesseract could not be found.\n",
      "25/05/14 09:35:41 WARN DAGScheduler: Broadcasting large task binary with size 1255.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"binaryFile\").load(SOURCE_PDF_PATH)\n",
    "result = pipe.fit(df).transform(df)\n",
    "result.write.format('parquet').mode('overwrite').save(DF_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f54cce-e573-466f-8c12-f9489ed3d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PDF_Deid_Deidentification_Medium_9.pdf | Precision: 0.9375 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_7.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_8.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_6.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_2.pdf | Precision: 0.9375 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_4.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_0.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_1.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Medium_3.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "\n",
      "Overall Precision: 0.9861\n",
      "Overall Recall: 1.0\n",
      "F1 Score: 0.993\n",
      "Mapping File Saved To : /workspace/zero_shot_result_mapping.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(SOURCE_GT_PATH=SOURCE_GT_PATH, \n",
    "                     DF_SAVE_PATH=DF_SAVE_PATH, \n",
    "                     SAVE_MAPPING_PATH=SAVE_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9006a728-6ca5-443a-959b-5b0c5a6eb830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 124:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "OBFUSCATED_IMAGE_COL = \"image_with_regions\"\n",
    "\n",
    "img_to_pdf = ImageToPdf() \\\n",
    "    .setPageNumCol(\"pagenum\") \\\n",
    "    .setOriginCol(\"path\") \\\n",
    "    .setOutputCol(\"pdf\") \\\n",
    "    .setInputCol(OBFUSCATED_IMAGE_COL) \\\n",
    "    .setAggregatePages(True)\n",
    "\n",
    "source = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "result_pdf = img_to_pdf.transform(source)\n",
    "\n",
    "for row in result_pdf.select(\"path\", \"pdf\").toLocalIterator():\n",
    "  filename = row.asDict()[\"path\"]\n",
    "  basename = os.path.basename(filename)\n",
    "\n",
    "  savename = os.path.join(SAVE_OUTPUT_PDF, basename)\n",
    "    \n",
    "  pdfFile = open(savename, \"wb\")\n",
    "  pdfFile.write(row.asDict()[\"pdf\"])\n",
    "  pdfFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
