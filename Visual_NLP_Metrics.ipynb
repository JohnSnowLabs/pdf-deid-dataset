{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a5b216-140e-451f-9d66-f96417b5c0d9",
   "metadata": {},
   "source": [
    "## Spark Session Setup for Visual NLP, Healthcare NLP, and Open-Source NLP\n",
    "\n",
    "To use this notebook, you need to start a Spark session with the following John Snow Labs libraries:\n",
    "\n",
    "- **[Spark-OCR / Visual-NLP](https://nlp.johnsnowlabs.com/docs/en/ocr)**\n",
    "- **[Healthcare NLP](https://nlp.johnsnowlabs.com/licensed/api/python/)**\n",
    "- **[Open-Source NLP](https://github.com/JohnSnowLabs/spark-nlp)**\n",
    "\n",
    "### Required Environment Variables\n",
    "\n",
    "Ensure you have a valid license file containing your credentials. The following environment variables must be set:\n",
    "\n",
    "- `SPARK_NLP_LICENSE` (Healthcare)\n",
    "- `SECRET` (Healthcare)\n",
    "- `JSL_VERSION` (Healthcare)\n",
    "- `SPARK_OCR_LICENSE` (Visual)\n",
    "- `SPARK_OCR_SECRET` (Visual)\n",
    "- `OCR_VERSION` (Visual)\n",
    "- `PUBLIC_VERSION` (Open-Source)\n",
    "- `AWS_ACCESS_KEY_ID`\n",
    "- `AWS_SECRET_ACCESS_KEY`\n",
    "- `AWS_SESSION_TOKEN`\n",
    "\n",
    "### Notes\n",
    "\n",
    "- For **text-only projects** (i.e., no visual data processing), you can use **`SPARK_NLP_LICENSE`**.\n",
    "- For projects involving **visual data** (e.g., image or PDF processing), you should use **`SPARK_OCR_LICENSE`**.\n",
    "- For projects involving both **visual** and **text** you can use either one of them.\n",
    "- All required key-value pairs **must be set as environment variables** to install and use the full functionality of these libraries.\n",
    "- Ensure that you **restart** the session after installing all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ec7b31-e13a-44d6-a0ea-a7438631eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "license = \"\"\n",
    "\n",
    "if license and \"json\" in license:\n",
    "\n",
    "    with open(license, \"r\") as creds_in:\n",
    "        creds = json.loads(creds_in.read())\n",
    "\n",
    "        for key in creds.keys():\n",
    "            os.environ[key] = creds[key]\n",
    "else:\n",
    "    raise Exception(\"License JSON File is not specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32203195-5897-4ad7-851d-f5a94d1f2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spark-ocr 6.0.0 requires spark-nlp==5.5.3, but you have spark-nlp 6.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q https://pypi.johnsnowlabs.com/$SPARK_OCR_SECRET/spark-ocr/spark_ocr-$OCR_VERSION-py3-none-any.whl\n",
    "\n",
    "!pip install --upgrade -q https://pypi.johnsnowlabs.com/$SECRET/spark-nlp-jsl/spark_nlp_jsl-$JSL_VERSION-py3-none-any.whl\n",
    "\n",
    "!pip install -q spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "!pip install -q pandas\n",
    "\n",
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75494ab3-1e9b-4e56-b841-3489cf7e1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESTART SESSION!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9382e-08df-42d5-bdd1-2e23afc1f4ff",
   "metadata": {},
   "source": [
    "## Start Spark Session - Visual NLP, Healthcare NLP, Spark-NLP\n",
    "\n",
    "In this section, we initialize the Spark session using the `start()` function from the **`sparkocr`** package.\n",
    "\n",
    "This utility sets up a fully configured Spark session tailored for **Spark OCR** and optionally for **Spark NLP**, **Healthcare NLP**, and **GPU/Apple Silicon support**.\n",
    "\n",
    "### Function Overview: `start()`\n",
    "\n",
    "The `start()` function returns a ready-to-use `SparkSession` and accepts the following parameters:\n",
    "\n",
    "- **`secret`**: Secret key required to download JAR files from the John Snow Labs server.\n",
    "- **`jar_path`**: (Optional) Local path to a pre-downloaded JAR file.\n",
    "- **`extra_conf`**: Additional Spark configuration â€” can be a `SparkConf` object or a Python `dict`.\n",
    "- **`master_url`**: URL for the Spark master (e.g., `\"local[*]\"`).\n",
    "- **`nlp_version`**: Version of Spark NLP to use. If `None`, Spark NLP is not included.\n",
    "- **`nlp_internal`**: Boolean indicating whether to include Spark NLP Internal.\n",
    "- **`nlp_jsl`**: Boolean or version string to include Spark NLP for Healthcare (JSL).\n",
    "- **`nlp_secret`**: Secret key for downloading Spark NLP Internal.\n",
    "- **`m1`**: Set to `True` to enable support for Apple Silicon (M1/M2) Macs.\n",
    "- **`keys_file`**: Path to a JSON file containing your credentials. Default is `'keys.json'`.\n",
    "- **`logLevel`**: Logging level for Spark (e.g., `\"WARN\"`, `\"INFO\"`).\n",
    "- **`use_gpu`**: Whether to enable GPU support for Spark NLP. Default is `False`.\n",
    "- **`apple_silicon`**: Whether to use Apple Silicon binaries. Default is `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8c5d2-49da-408a-a151-c0474bf9bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkocr import start\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "license = \"\"\n",
    "\n",
    "extra_configurations = {\n",
    "    \"spark.extraListeners\": \"com.johnsnowlabs.license.LicenseLifeCycleManager\", #required\n",
    "    \"spark.sql.legacy.allowUntypedScalaUDF\" : \"true\", #required\n",
    "    \"spark.executor.instances\" : \"7\", \n",
    "    \"spark.executor.cores\" : \"16\", \n",
    "    \"spark.executor.memory\" : \"130G\", \n",
    "    \"spark.driver.memory\" : \"100G\", \n",
    "    \"spark.sql.shuffle.partitions\" : \"896\"\n",
    "}\n",
    "\n",
    "# Not needed for Google Collab\n",
    "os.environ['JAVA_HOME'] = '/home/linuxbrew/.linuxbrew/Cellar/openjdk@17/17.0.15'\n",
    "\n",
    "spark = start(nlp_internal=True,\n",
    "              nlp_jsl=True,\n",
    "              use_gpu=False,\n",
    "              extra_conf=extra_configurations,\n",
    "              keys_file=license)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8362bb4-14c8-4ada-89ca-3ba15bb1a2df",
   "metadata": {},
   "source": [
    "<h2>Import Visual NLP, Healthcare NLP and Spark-NLP</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61d21ee-84b0-4846-b73d-66846b8f3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Pyspark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Necessary imports from Spark OCR library\n",
    "import sparkocr\n",
    "from sparkocr import start\n",
    "from sparkocr.transformers import *\n",
    "from sparkocr.enums import *\n",
    "from sparkocr.utils import *\n",
    "\n",
    "# import sparknlp packages\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "from collections import Counter\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6b6905-7d7f-425b-b71c-b60b28d15b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(SOURCE_GT_PATH, DF_SAVE_PATH, SAVE_MAPPING_PATH):\n",
    "    \"\"\"\n",
    "    Method to Calculate Precision, Recall and F1-Score\n",
    "    Saves final file with prediction, ground truth, precision, recall\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_metrics(preds, gts):\n",
    "      gt_counter = Counter(gts)\n",
    "      pred_counter = Counter(preds)\n",
    "\n",
    "      tp = 0\n",
    "      for item in pred_counter:\n",
    "          if item in gt_counter:\n",
    "              tp += min(pred_counter[item], gt_counter[item])\n",
    "\n",
    "      fp = sum(pred_counter.values()) - tp\n",
    "      fn = sum(gt_counter.values()) - tp\n",
    "\n",
    "      precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "      recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "\n",
    "      return precision, recall\n",
    "\n",
    "    with open(SOURCE_GT_PATH, \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "\n",
    "    df_predictions = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "\n",
    "    predictions_by_file = {}\n",
    "\n",
    "    for row in df_predictions.select(\"path\").distinct().toLocalIterator():\n",
    "        file_path = row.asDict()[\"path\"]\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        if filename not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        extracted_results = []\n",
    "        rows = df_predictions.filter(F.col(\"path\") == file_path).select(\"positions_ner\")\n",
    "\n",
    "        for r in rows.toLocalIterator():\n",
    "            for ner in r.asDict()[\"positions_ner\"]:\n",
    "                extracted_results.append(ner.asDict()[\"result\"])\n",
    "\n",
    "        predictions_by_file[filename] = extracted_results\n",
    "\n",
    "    summary = {}\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "\n",
    "    for filename, predictions in predictions_by_file.items():\n",
    "        gt_values = ground_truth[filename]\n",
    "        gt_values = [i.replace(\"-year-old\", \"\") for i in gt_values]\n",
    "        predictions = [i.replace(\"-year-old\", \"\") for i in predictions]\n",
    "        precision, recall = calculate_metrics([i.replace(\" \", \"\") for i in predictions], \n",
    "                                              [i.replace(\" \", \"\") for i in gt_values])\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        summary[filename] = {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"gt\": gt_values,\n",
    "            \"pred\": predictions\n",
    "        }\n",
    "\n",
    "        print(f\"Filename: {filename} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    avg_precision = round(sum(all_precisions) / len(all_precisions), 4)\n",
    "    avg_recall = round(sum(all_recalls) / len(all_recalls), 4)\n",
    "    f1_score = round(2 * (avg_precision * avg_recall) / (avg_precision + avg_recall), 4)\n",
    "\n",
    "    print(f\"\\nOverall Precision: {avg_precision}\")\n",
    "    print(f\"Overall Recall: {avg_recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "    with open(SAVE_MAPPING_PATH, \"w\") as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    print(f\"Mapping File Saved To : {SAVE_MAPPING_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0db616-3584-48be-8cb6-85c44850c91b",
   "metadata": {},
   "source": [
    "## Threshold Parameters\n",
    "\n",
    "- **`ner_threshold`**  \n",
    "  Filters out entities based on their predicted confidence scores. This parameter is used in the `NerConverterInternal` stage to retain only high-confidence predictions.\n",
    "\n",
    "- **`ocr_threshold`**  \n",
    "  Filters out predicted text from OCR based on confidence scores, ensuring only reliable OCR outputs are used downstream.\n",
    "\n",
    "- **`matcherWhitelist`**  \n",
    "  Applies text matching to identify and retain similar entities from predictions, guided by a confidence score threshold.\n",
    "\n",
    "- **`whitelist`**  \n",
    "  Allows you to retain only specific entity classes by explicitly listing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7595bef5-603c-4a0f-9ec1-94ec9523677c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOSPITAL': 0.9,\n",
       " 'NAME': 0.6,\n",
       " 'PATIENT': 0.9,\n",
       " 'ID': 0.9,\n",
       " 'MEDICALRECORD': 0.6,\n",
       " 'IDNUM': 0.6,\n",
       " 'COUNTRY': 0.9,\n",
       " 'LOCATION': 0.9,\n",
       " 'STREET': 0.9,\n",
       " 'STATE': 0.9,\n",
       " 'ZIP': 0.9,\n",
       " 'CONTACT': 0.9,\n",
       " 'PHONE': 0.9,\n",
       " 'DATE': 0.9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ner Threshold\n",
    "ner_threshold = 0.90\n",
    "\n",
    "# OCR Output Threshold\n",
    "ocr_threshold = 70\n",
    "\n",
    "# Ner Whitelist Entites\n",
    "whitelist = ['HOSPITAL', 'NAME', 'PATIENT', 'ID', 'MEDICALRECORD', 'IDNUM', 'COUNTRY', 'LOCATION', 'STREET', 'STATE', 'ZIP', 'CONTACT', 'PHONE', 'DATE']\n",
    "\n",
    "# Matcher is used for regex matching from already detected NER\n",
    "# NER threshold is used to select detected NER for matching\n",
    "matcherWhitelist = {'HOSPITAL': 0.9,\n",
    " 'NAME': 0.6,\n",
    " 'PATIENT': 0.9,\n",
    " 'ID': 0.9,\n",
    " 'MEDICALRECORD': 0.6,\n",
    " 'IDNUM': 0.6,\n",
    " 'COUNTRY': 0.9,\n",
    " 'LOCATION': 0.9,\n",
    " 'STREET': 0.9,\n",
    " 'STATE': 0.9,\n",
    " 'ZIP': 0.9,\n",
    " 'CONTACT': 0.9,\n",
    " 'PHONE': 0.9,\n",
    " 'DATE': 0.9}\n",
    "\n",
    "matcherWhitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3c41d-3b45-4906-91ce-2de4fdc325a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_to_image = PdfToImage() \\\n",
    "  .setInputCol(\"content\") \\\n",
    "  .setSplitNumBatch(10) \\\n",
    "  .setOutputCol(\"image_raw\") \\\n",
    "  .setImageType(ImageType.TYPE_3BYTE_BGR) \\\n",
    "  .setSplittingStategy(SplittingStrategy.FIXED_NUMBER_OF_PARTITIONS)\n",
    "\n",
    "ocr = ImageToText() \\\n",
    "    .setInputCol(\"image_raw\") \\\n",
    "    .setOutputCol(\"text\") \\\n",
    "    .setIgnoreResolution(False) \\\n",
    "    .setPageIteratorLevel(PageIteratorLevel.SYMBOL) \\\n",
    "    .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) \\\n",
    "    .setWithSpaces(True) \\\n",
    "    .setKeepLayout(False) \\\n",
    "    .setConfidenceThreshold(70)\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink_full\") \n",
    "\n",
    "abbreviations = ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n",
    "sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setImpossiblePenultimates(abbreviations) \\\n",
    "    .setUseCustomBoundsOnly(False) \\\n",
    "    .setSplitLength(2147483647) \\\n",
    "    .setExplodeSentences(False)\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setContextChars(['.', ',', ';', ':', '!', '?', '*', '\"', \"'\"])\n",
    "\n",
    "\n",
    "ner_docwise_large = PretrainedZeroShotNER().pretrained(\"zeroshot_ner_deid_subentity_docwise_large\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols(\"sentence\", \"token\") \\\n",
    "    .setOutputCol(\"ner_docwise_large\") \\\n",
    "    .setLabels([\"CITY\", \"COUNTRY\", \"PHONE\", \"IDNUM\", \"ID\", \"MEDICALRECORD\", \"DATE\", \"HOSPITAL\", \"ORGANIZATION\", \"STATE\", \"STREET\"])\n",
    "\n",
    "ner_chunk_docwise_large = NerConverterInternal() \\\n",
    "    .setInputCols(\"sentence\", \"token\", \"ner_docwise_large\") \\\n",
    "    .setOutputCol(\"ner_chunk_docwise_large\") \\\n",
    "    .setThreshold(0.90)\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_deid = MedicalNerModel.pretrained(\"ner_deid_subentity_augmented_docwise\", \"en\", \"clinical/models\")  \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner_deid_subentity_augmented_docwise\")\n",
    "\n",
    "ner_deid_converter = NerConverterInternal()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner_deid_subentity_augmented_docwise\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_subentity_augmented_docwise\") \\\n",
    "    .setWhiteList([\"IDNUM\", \"MEDICALRECORD\", \"ZIP\"]) \\\n",
    "    .setThreshold(0.60)\n",
    "\n",
    "embeddings = XlmRoBertaEmbeddings.pretrained(\"xlm_roberta_base\", \"xx\") \\\n",
    "    .setInputCols(\"sentence\", \"token\") \\\n",
    "    .setOutputCol(\"xlm_embeddings\")\\\n",
    "    .setMaxSentenceLength(512)\\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "ner = MedicalNerModel.pretrained(\"ner_deid_name_multilingual\", \"xx\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"xlm_embeddings\"]) \\\n",
    "    .setOutputCol(\"ner_deid_name_multilingual\")\n",
    "\n",
    "ner_name_converter = NerConverterInternal()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner_deid_name_multilingual\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_name_multilingual\") \\\n",
    "    .setThreshold(0.60)\n",
    "\n",
    "age_contextual_parser = ContextualParserModel.pretrained(\"age_parser\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_age\")\n",
    "\n",
    "age_chunk_converter = ChunkConverter() \\\n",
    "    .setInputCols([\"chunk_age\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_age\")\n",
    "\n",
    "chunk_merger = ChunkMergeApproach() \\\n",
    "    .setInputCols('ner_chunk_subentity_augmented_docwise', 'ner_chunk_docwise_large', 'ner_chunk_name_multilingual', 'ner_chunk_age') \\\n",
    "    .setOutputCol('merged_ner_chunk') \\\n",
    "    .setMergeOverlapping(True)\n",
    "\n",
    "deid_obfuscated = DeIdentification() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"merged_ner_chunk\"]) \\\n",
    "    .setOutputCol(\"obfuscated\") \\\n",
    "    .setMode(\"obfuscate\") \\\n",
    "    .setKeepMonth(True) \\\n",
    "    .setKeepYear(True) \\\n",
    "    .setObfuscateDate(True) \\\n",
    "    .setSameEntityThreshold(0.7) \\\n",
    "    .setKeepTextSizeForObfuscation(True) \\\n",
    "    .setFakerLengthOffset(2) \\\n",
    "    .setReturnEntityMappings(True) \\\n",
    "    .setDays(2) \\\n",
    "    .setMappingsColumn(\"aux\") \\\n",
    "    .setIgnoreRegex(True) \\\n",
    "    .setGroupByCol(\"path\") \\\n",
    "    .setRegion(\"us\") \\\n",
    "    .setSeed(40) \\\n",
    "    .setConsistentObfuscation(True) \\\n",
    "    .setChunkMatching(matcherWhitelist)\n",
    "\n",
    "cleaner = NerOutputCleaner() \\\n",
    "    .setInputCol(\"aux\") \\\n",
    "    .setOutputCol(\"new_aux\") \\\n",
    "    .setOutputNerCol(\"positions_ner\")\n",
    "\n",
    "position_finder = PositionFinder() \\\n",
    "    .setInputCols(\"positions_ner\") \\\n",
    "    .setOutputCol(\"coordinates\") \\\n",
    "    .setPageMatrixCol(\"positions\")\n",
    "\n",
    "draw_regions = ImageDrawRegions() \\\n",
    "  .setInputCol(\"image_raw\") \\\n",
    "  .setInputRegionsCol(\"coordinates\") \\\n",
    "  .setRectColor(Color.black) \\\n",
    "  .setFilledRect(True) \\\n",
    "  .setOutputCol(\"image_with_regions\")\n",
    "\n",
    "stages = [\n",
    "    pdf_to_image,\n",
    "    ocr,\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    ner_docwise_large,\n",
    "    ner_chunk_docwise_large,\n",
    "    word_embeddings,\n",
    "    ner_deid,\n",
    "    ner_deid_converter,\n",
    "    embeddings,\n",
    "    ner,\n",
    "    ner_name_converter,\n",
    "    age_contextual_parser,\n",
    "    age_chunk_converter,\n",
    "    chunk_merger,\n",
    "    deid_obfuscated,\n",
    "    cleaner,\n",
    "    position_finder,\n",
    "    draw_regions\n",
    "]\n",
    "\n",
    "pipe = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7469c8-dca6-43f7-b954-6c46e8afdc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PdfToImage_24383e7d72af,\n",
       " ImageToText_777a0e572b06,\n",
       " DocumentAssembler_334bbe45c83c,\n",
       " SentenceDetectorDLModel_c83c27f46b97,\n",
       " Tokenizer_cb576e40ba33,\n",
       " PretrainedZeroShotNER_ca8c4dfe310f,\n",
       " NerConverterInternal_f8dcb764cea1,\n",
       " WORD_EMBEDDINGS_MODEL_9004b1d00302,\n",
       " MedicalNerModel_ada39ac0d359,\n",
       " NerConverterInternal_0da57797b1b8,\n",
       " XLM_ROBERTA_EMBEDDINGS_b8a75c006754,\n",
       " MedicalNerModel_59183c57aedb,\n",
       " NerConverterInternal_6a42cb6c30fa,\n",
       " CONTEXTUAL-PARSER_100152bbc72d,\n",
       " ChunkConverter_ed78f9d4128b,\n",
       " ChunkMergeApproach_71a45f6027ab,\n",
       " DeIdentification_1802897a3390,\n",
       " NerOutputCleaner_817bf8a3d210,\n",
       " PositionFinder_d370170be46b,\n",
       " ImageDrawRegions_a15485c93c74]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4ade3-fb7e-4bd4-a93a-35a646854549",
   "metadata": {},
   "source": [
    "## File and Directory Paths\n",
    "\n",
    "- **`SOURCE_PDF_PATH`**  \n",
    "  Path to the local folder containing input PDF files.\n",
    "\n",
    "- **`DF_SAVE_PATH`**  \n",
    "  Path for saving intermediate DataFrame results. Useful for checkpointing across multiple pipeline stages.\n",
    "\n",
    "- **`SOURCE_GT_PATH`**  \n",
    "  Path to the local folder containing JSON files with ground truth information.\n",
    "\n",
    "- **`SAVE_MAPPING_PATH`**  \n",
    "  Path to the JSON file where evaluation results (including predicted vs. ground truth, precision, recall, and F1-score) will be stored.\n",
    "\n",
    "- **`SAVE_OUTPUT_PDF`**  \n",
    "  Path to the folder where the output (redacted or annotated) PDF files will be saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160978c-7b10-463b-88f8-842f2abff1e0",
   "metadata": {},
   "source": [
    "<h2>Easy Dataset</h2>\n",
    "\n",
    "<h4>Total Files : [ 30 Files ]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678ccd65-c11b-4080-8907-af5e08a38357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SOURCE_PDF_PATH = [\"./PDF_Original/Easy/\"]\n",
    "DF_SAVE_PATH = \"./df_temp/easy/\" #should be regenerated\n",
    "SOURCE_GT_PATH = \"./Mapping/all_phi/pdf_deid_gts_easy.json\"\n",
    "SAVE_MAPPING_PATH = \"./Mapping/all_phi/easy_result_mapping.json\"\n",
    "SAVE_OUTPUT_PDF = \"./easy_pdf_output/\"\n",
    "\n",
    "os.makedirs(SAVE_OUTPUT_PDF, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736cbc6-8139-4e19-aa15-ccbd36b5b886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\") \\\n",
    "    .load(SOURCE_PDF_PATH) \\\n",
    "    .filter(~col(\"path\").contains(\"ipynb\"))\n",
    "\n",
    "result = pipe.fit(df).transform(df)\n",
    "result.write.format('parquet').mode('overwrite').save(DF_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ae604e-92f1-41b0-83f2-f296ab2f117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PDF_Deid_Deidentification_2.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_22.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_6.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_9.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_5.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_8.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_1.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_27.pdf | Precision: 1.0000 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_28.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_0.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_19.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_11.pdf | Precision: 0.9762 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_17.pdf | Precision: 1.0000 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_4.pdf | Precision: 0.9500 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_25.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_10.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_7.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_24.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_21.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_29.pdf | Precision: 1.0000 | Recall: 0.9744\n",
      "Filename: PDF_Deid_Deidentification_14.pdf | Precision: 0.8421 | Recall: 0.9697\n",
      "Filename: PDF_Deid_Deidentification_3.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_16.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_20.pdf | Precision: 0.9512 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_23.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_15.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_26.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_13.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_18.pdf | Precision: 1.0000 | Recall: 0.8205\n",
      "Filename: PDF_Deid_Deidentification_12.pdf | Precision: 0.9744 | Recall: 0.9744\n",
      "\n",
      "Overall Precision: 0.9851\n",
      "Overall Recall: 0.9799\n",
      "F1 Score: 0.9825\n",
      "Mapping File Saved To : ./Mapping/all_phi/pdf_deid_gts_easy.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(SOURCE_GT_PATH=SOURCE_GT_PATH, \n",
    "                     DF_SAVE_PATH=DF_SAVE_PATH, \n",
    "                     SAVE_MAPPING_PATH=SAVE_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df202bae-e1c6-48a0-b0e1-7f1d4f71c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 515:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "OBFUSCATED_IMAGE_COL = \"image_with_regions\"\n",
    "\n",
    "img_to_pdf = ImageToPdf() \\\n",
    "    .setPageNumCol(\"pagenum\") \\\n",
    "    .setOriginCol(\"path\") \\\n",
    "    .setOutputCol(\"pdf\") \\\n",
    "    .setInputCol(OBFUSCATED_IMAGE_COL) \\\n",
    "    .setAggregatePages(True)\n",
    "\n",
    "source = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "result_pdf = img_to_pdf.transform(source)\n",
    "\n",
    "for row in result_pdf.select(\"path\", \"pdf\").toLocalIterator():\n",
    "  filename = row.asDict()[\"path\"]\n",
    "  basename = os.path.basename(filename)\n",
    "\n",
    "  savename = os.path.join(SAVE_OUTPUT_PDF, basename)\n",
    "    \n",
    "  pdfFile = open(savename, \"wb\")\n",
    "  pdfFile.write(row.asDict()[\"pdf\"])\n",
    "  pdfFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e124b5-744f-4057-840d-0885e8463d2e",
   "metadata": {},
   "source": [
    "<h2>Medium Dataset</h2>\n",
    "\n",
    "<h4>Total Files : [ 40 Files ( 30 Easy + 10 Medium ) ]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6da053e-e72e-4f55-ae76-bd1422d96421",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PDF_PATH = [\"./PDF_Original/Easy/\", \"./PDF_Original/Medium/\",]\n",
    "DF_SAVE_PATH = \"./df_temp/medium/\" #should be regenerated\n",
    "SOURCE_GT_PATH = \"./Mapping/all_phi/pdf_deid_gts_medium.json\"\n",
    "SAVE_MAPPING_PATH = \"./Mapping/all_phi/medium_result_mapping.json\"\n",
    "SAVE_OUTPUT_PDF = \"./medium_pdf_output/\"\n",
    "\n",
    "os.makedirs(SAVE_OUTPUT_PDF, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c19a6-62c5-4f03-9795-829627ca2b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\") \\\n",
    "    .load(SOURCE_PDF_PATH) \\\n",
    "    .filter(~col(\"path\").contains(\"ipynb\"))\n",
    "\n",
    "result = pipe.fit(df).transform(df)\n",
    "result.write.format('parquet').mode('overwrite').save(DF_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d121ff8-5b31-49be-8787-9e27790a8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PDF_Deid_Deidentification_Medium_5.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_6.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_8.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_9.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_3.pdf | Precision: 1.0000 | Recall: 0.8846\n",
      "Filename: PDF_Deid_Deidentification_Medium_1.pdf | Precision: 0.9200 | Recall: 0.8846\n",
      "Filename: PDF_Deid_Deidentification_Medium_2.pdf | Precision: 0.9333 | Recall: 0.8077\n",
      "Filename: PDF_Deid_Deidentification_Medium_0.pdf | Precision: 0.9796 | Recall: 0.9231\n",
      "Filename: PDF_Deid_Deidentification_Medium_4.pdf | Precision: 0.9800 | Recall: 0.9423\n",
      "Filename: PDF_Deid_Deidentification_Medium_7.pdf | Precision: 0.9167 | Recall: 0.8462\n",
      "Filename: PDF_Deid_Deidentification_16.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_20.pdf | Precision: 0.9512 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_17.pdf | Precision: 1.0000 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_4.pdf | Precision: 0.9500 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_14.pdf | Precision: 0.8421 | Recall: 0.9697\n",
      "Filename: PDF_Deid_Deidentification_3.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_5.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_8.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_28.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_0.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_26.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_13.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_21.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_29.pdf | Precision: 1.0000 | Recall: 0.9744\n",
      "Filename: PDF_Deid_Deidentification_7.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_11.pdf | Precision: 0.9762 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_25.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_10.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_18.pdf | Precision: 1.0000 | Recall: 0.8205\n",
      "Filename: PDF_Deid_Deidentification_12.pdf | Precision: 0.9744 | Recall: 0.9744\n",
      "Filename: PDF_Deid_Deidentification_24.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_1.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_27.pdf | Precision: 1.0000 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_23.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_15.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_6.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_2.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_22.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_9.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_19.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "\n",
      "Overall Precision: 0.98\n",
      "Overall Recall: 0.9575\n",
      "F1 Score: 0.9686\n",
      "Mapping File Saved To : ./Mapping/all_phi/medium_result_mapping.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(SOURCE_GT_PATH=SOURCE_GT_PATH, \n",
    "                     DF_SAVE_PATH=DF_SAVE_PATH, \n",
    "                     SAVE_MAPPING_PATH=SAVE_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8194b3da-133d-4732-81a4-5a70c29dccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 668:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "OBFUSCATED_IMAGE_COL = \"image_with_regions\"\n",
    "\n",
    "img_to_pdf = ImageToPdf() \\\n",
    "    .setPageNumCol(\"pagenum\") \\\n",
    "    .setOriginCol(\"path\") \\\n",
    "    .setOutputCol(\"pdf\") \\\n",
    "    .setInputCol(OBFUSCATED_IMAGE_COL) \\\n",
    "    .setAggregatePages(True)\n",
    "\n",
    "source = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "result_pdf = img_to_pdf.transform(source)\n",
    "\n",
    "for row in result_pdf.select(\"path\", \"pdf\").toLocalIterator():\n",
    "  filename = row.asDict()[\"path\"]\n",
    "  basename = os.path.basename(filename)\n",
    "\n",
    "  savename = os.path.join(SAVE_OUTPUT_PDF, basename)\n",
    "    \n",
    "  if \"Medium\" in filename:\n",
    "      pdfFile = open(savename, \"wb\")\n",
    "      pdfFile.write(row.asDict()[\"pdf\"])\n",
    "      pdfFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf7b0f-14f8-4293-9e10-5749943bc328",
   "metadata": {},
   "source": [
    "<h2>Hard Dataset</h2>\n",
    "\n",
    "<h4>Total Files : [ 50 Files ( 30 Easy + 10 Medium + 10 Hard) ]</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b321e21-3a5a-4cfd-b120-e9ff219abd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PDF_PATH = [\"./PDF_Original/Easy/\", \"./PDF_Original/Medium/\",  \"./PDF_Original/Hard/\",]\n",
    "DF_SAVE_PATH = \"./df_temp/hard/\" #should be regenerated\n",
    "SOURCE_GT_PATH = \"./Mapping/all_phi/pdf_deid_gts_hard.json\"\n",
    "SAVE_MAPPING_PATH = \"./Mapping/all_phi/hard_result_mapping.json\"\n",
    "SAVE_OUTPUT_PDF = \"./hard_pdf_output/\"\n",
    "\n",
    "os.makedirs(SAVE_OUTPUT_PDF, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1ad1d-0811-4cec-b001-574adc6aebe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\") \\\n",
    "    .load(SOURCE_PDF_PATH) \\\n",
    "    .filter(~col(\"path\").contains(\"ipynb\"))\n",
    "\n",
    "result = pipe.fit(df).transform(df)\n",
    "result.write.format('parquet').mode('overwrite').save(DF_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a5f9dd2-9bec-4264-a513-59ede8f86e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: PDF_Deid_Deidentification_Medium_9.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_5.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_0.pdf | Precision: 0.9796 | Recall: 0.9231\n",
      "Filename: PDF_Deid_Deidentification_Medium_2.pdf | Precision: 0.9333 | Recall: 0.8077\n",
      "Filename: PDF_Deid_Deidentification_Medium_8.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_1.pdf | Precision: 0.9200 | Recall: 0.8846\n",
      "Filename: PDF_Deid_Deidentification_Medium_4.pdf | Precision: 0.9800 | Recall: 0.9423\n",
      "Filename: PDF_Deid_Deidentification_Medium_7.pdf | Precision: 0.9167 | Recall: 0.8462\n",
      "Filename: PDF_Deid_Deidentification_Medium_6.pdf | Precision: 0.9792 | Recall: 0.9038\n",
      "Filename: PDF_Deid_Deidentification_Medium_3.pdf | Precision: 1.0000 | Recall: 0.8846\n",
      "Filename: PDF_Deid_Deidentification_16.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_20.pdf | Precision: 0.9512 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_17.pdf | Precision: 1.0000 | Recall: 0.9512\n",
      "Filename: PDF_Deid_Deidentification_4.pdf | Precision: 0.9500 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_14.pdf | Precision: 0.8421 | Recall: 0.9697\n",
      "Filename: PDF_Deid_Deidentification_3.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_8.pdf | Precision: 0.8222 | Recall: 0.8043\n",
      "Filename: PDF_Deid_Deidentification_5.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_8.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_26.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_13.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_0.pdf | Precision: 0.8780 | Recall: 0.7826\n",
      "Filename: PDF_Deid_Deidentification_21.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_29.pdf | Precision: 1.0000 | Recall: 0.9744\n",
      "Filename: PDF_Deid_Deidentification_7.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_23.pdf | Precision: 0.9756 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_15.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_6.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_7.pdf | Precision: 0.8889 | Recall: 0.8696\n",
      "Filename: PDF_Deid_Deidentification_1.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_3.pdf | Precision: 0.8409 | Recall: 0.8043\n",
      "Filename: PDF_Deid_Deidentification_27.pdf | Precision: 1.0000 | Recall: 0.9268\n",
      "Filename: PDF_Deid_Deidentification_2.pdf | Precision: 1.0000 | Recall: 0.9756\n",
      "Filename: PDF_Deid_Deidentification_Hard_4.pdf | Precision: 0.9024 | Recall: 0.8043\n",
      "Filename: PDF_Deid_Deidentification_22.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_24.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_2.pdf | Precision: 0.8605 | Recall: 0.8222\n",
      "Filename: PDF_Deid_Deidentification_Hard_6.pdf | Precision: 0.8409 | Recall: 0.8043\n",
      "Filename: PDF_Deid_Deidentification_28.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_0.pdf | Precision: 0.9535 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_1.pdf | Precision: 0.8444 | Recall: 0.8261\n",
      "Filename: PDF_Deid_Deidentification_11.pdf | Precision: 0.9762 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_25.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_10.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_18.pdf | Precision: 1.0000 | Recall: 0.8205\n",
      "Filename: PDF_Deid_Deidentification_12.pdf | Precision: 0.9744 | Recall: 0.9744\n",
      "Filename: PDF_Deid_Deidentification_9.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_9.pdf | Precision: 0.9048 | Recall: 0.8261\n",
      "Filename: PDF_Deid_Deidentification_19.pdf | Precision: 1.0000 | Recall: 1.0000\n",
      "Filename: PDF_Deid_Deidentification_Hard_5.pdf | Precision: 0.8222 | Recall: 0.8043\n",
      "\n",
      "Overall Precision: 0.9561\n",
      "Overall Recall: 0.929\n",
      "F1 Score: 0.9424\n",
      "Mapping File Saved To : ./Mapping/all_phi/hard_result_mapping.json\n"
     ]
    }
   ],
   "source": [
    "evaluate_predictions(SOURCE_GT_PATH=SOURCE_GT_PATH, \n",
    "                     DF_SAVE_PATH=DF_SAVE_PATH, \n",
    "                     SAVE_MAPPING_PATH=SAVE_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f48caaa-2b2e-4468-8d5e-050a12fbe315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5963:>                                                       (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "OBFUSCATED_IMAGE_COL = \"image_with_regions\"\n",
    "\n",
    "img_to_pdf = ImageToPdf() \\\n",
    "    .setPageNumCol(\"pagenum\") \\\n",
    "    .setOriginCol(\"path\") \\\n",
    "    .setOutputCol(\"pdf\") \\\n",
    "    .setInputCol(OBFUSCATED_IMAGE_COL) \\\n",
    "    .setAggregatePages(True)\n",
    "\n",
    "source = spark.read.format(\"parquet\").load(DF_SAVE_PATH)\n",
    "result_pdf = img_to_pdf.transform(source)\n",
    "\n",
    "for row in result_pdf.select(\"path\", \"pdf\").toLocalIterator():\n",
    "  filename = row.asDict()[\"path\"]\n",
    "  basename = os.path.basename(filename)\n",
    "\n",
    "  savename = os.path.join(SAVE_OUTPUT_PDF, basename)\n",
    "    \n",
    "  if \"Hard\" in filename:\n",
    "      pdfFile = open(savename, \"wb\")\n",
    "      pdfFile.write(row.asDict()[\"pdf\"])\n",
    "      pdfFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
